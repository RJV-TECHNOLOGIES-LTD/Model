# ==============================================
# üöÄ INDUSTRY-LEADING CLOUD AUTO-SCALING FOR AI EXECUTION
# ‚úÖ Real-Time AI Workload Auto-Scaling, Self-Healing, Multi-Cloud Support
# ==============================================

import time
import psutil
import requests
import logging
from time import sleep

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')

# Cloud AI Model URL (Adjust based on cloud provider and AI endpoint)
CLOUD_AI_ENDPOINTS = {
    "aws": "https://aws-ai-endpoint.com/predict",
    "gcp": "https://gcp-ai-endpoint.com/predict",
    "azure": "https://azure-ai-endpoint.com/predict"
}

# Cloud Auto-Scaling Thresholds (In percentage)
CPU_THRESHOLD = 80  # Scale up if CPU usage exceeds 80%
MEMORY_THRESHOLD = 80  # Scale up if memory usage exceeds 80%
GPU_THRESHOLD = 80  # Scale up if GPU usage exceeds 80%

# Scaling parameters
MAX_INSTANCES = 10
MIN_INSTANCES = 1
SCALE_UP_COMMAND = "kubectl scale deployment unified-model-ai --replicas={}"
SCALE_DOWN_COMMAND = "kubectl scale deployment unified-model-ai --replicas={}"

# ---------------------------------------------------------------
# Function to monitor system resources (CPU, memory, GPU)
# ---------------------------------------------------------------
def monitor_system_resources():
    """Monitor system resources (CPU, Memory, GPU)"""
    cpu_usage = psutil.cpu_percent(interval=1)
    memory_usage = psutil.virtual_memory().percent

    # GPU usage (Example for NVIDIA GPU using nvidia-smi)
    try:
        gpu_usage = float(subprocess.check_output(["nvidia-smi", "--query-gpu=utilization.gpu", "--format=csv,noheader,nounits"]))
    except Exception as e:
        logging.error(f"‚ùå Error checking GPU usage: {e}")
        gpu_usage = 0

    logging.info(f"CPU Usage: {cpu_usage}% | Memory Usage: {memory_usage}% | GPU Usage: {gpu_usage}%")

    return cpu_usage, memory_usage, gpu_usage

# ---------------------------------------------------------------
# Function to trigger auto-scaling based on resource utilization
# ---------------------------------------------------------------
def trigger_auto_scaling(cpu_usage, memory_usage, gpu_usage):
    """Trigger auto-scaling based on real-time resource utilization."""
    # Scale up if resource usage exceeds thresholds
    if cpu_usage > CPU_THRESHOLD or memory_usage > MEMORY_THRESHOLD or gpu_usage > GPU_THRESHOLD:
        logging.info("‚ö†Ô∏è High resource usage detected! Triggering auto-scaling up.")
        # Scale up the AI execution instances
        scale_up_instances = min(MAX_INSTANCES, get_current_instances() + 1)  # Ensure we do not exceed max instances
        scale_up(scale_up_instances)
    elif cpu_usage < 20 and memory_usage < 20 and gpu_usage < 20:
        logging.info("‚ö†Ô∏è Low resource usage detected. Triggering auto-scaling down.")
        # Scale down AI execution instances
        scale_down_instances = max(MIN_INSTANCES, get_current_instances() - 1)  # Ensure we do not go below min instances
        scale_down(scale_down_instances)

# ---------------------------------------------------------------
# Function to get the current number of instances/pods
# ---------------------------------------------------------------
def get_current_instances():
    """Get the current number of running instances/pods for the AI model."""
    try:
        current_instances = int(subprocess.check_output("kubectl get deployment unified-model-ai -o=jsonpath='{.status.replicas}'", shell=True))
        return current_instances
    except subprocess.CalledProcessError as e:
        logging.error(f"‚ùå Error fetching current instances: {e}")
        return 0

# ---------------------------------------------------------------
# Function to scale AI instances/pods up
# ---------------------------------------------------------------
def scale_up(instances):
    """Scale up AI instances/pods."""
    logging.info(f"Scaling up AI execution to {instances} instances.")
    try:
        subprocess.run(SCALE_UP_COMMAND.format(instances), shell=True, check=True)
        logging.info(f"‚úÖ Successfully scaled up to {instances} instances.")
    except subprocess.CalledProcessError as e:
        logging.error(f"‚ùå Failed to scale up: {e}")

# ---------------------------------------------------------------
# Function to scale AI instances/pods down
# ---------------------------------------------------------------
def scale_down(instances):
    """Scale down AI instances/pods."""
    logging.info(f"Scaling down AI execution to {instances} instances.")
    try:
        subprocess.run(SCALE_DOWN_COMMAND.format(instances), shell=True, check=True)
        logging.info(f"‚úÖ Successfully scaled down to {instances} instances.")
    except subprocess.CalledProcessError as e:
        logging.error(f"‚ùå Failed to scale down: {e}")

# ---------------------------------------------------------------
# Function to monitor and adjust auto-scaling
# ---------------------------------------------------------------
def monitor_and_scale():
    """Monitor resource usage and adjust auto-scaling accordingly."""
    while True:
        try:
            logging.info("üîπ Monitoring AI execution and resources...")
            
            # Monitor system resources (CPU, memory, GPU)
            cpu_usage, memory_usage, gpu_usage = monitor_system_resources()

            # Trigger auto-scaling based on resource utilization
            trigger_auto_scaling(cpu_usage, memory_usage, gpu_usage)

            # Pause before the next monitoring cycle
            sleep(MONITOR_INTERVAL)

        except Exception as e:
            logging.error(f"‚ùå Error during auto-scaling process: {e}")
            sleep(MONITOR_INTERVAL)

if __name__ == "__main__":
    logging.info("üöÄ Starting Cloud Auto-Scaling for AI Execution...")

    # Monitor and auto-scale AI instances based on resource usage
    monitor_and_scale()

