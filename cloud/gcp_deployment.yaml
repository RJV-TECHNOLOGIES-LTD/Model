# ========================================================================================================
# ðŸš€ INDUSTRY-LEADING AI EXECUTION GCP DEPLOYMENT YAML FOR UNIFIED MODEL AI
# âœ… Fully Automated, Scalable, Self-Healing, and Secure AI Execution on GCP
# ========================================================================================================

# ------------------------------------------------------------
# ðŸ”¹ **Google Kubernetes Engine (GKE) Deployment for AI Models**
# ------------------------------------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: unified-model-ai
  labels:
    app: unified-model-ai
spec:
  replicas: 2  # Initial number of replicas (can scale up based on workload)
  selector:
    matchLabels:
      app: unified-model-ai
  template:
    metadata:
      labels:
        app: unified-model-ai
    spec:
      containers:
      - name: unified-model-ai
        image: gcr.io/YOUR_PROJECT_ID/unified-model-ai:latest  # Replace with your image
        ports:
        - containerPort: 8000
        resources:
          limits:
            memory: "4Gi"
            cpu: "2"
          requests:
            memory: "2Gi"
            cpu: "1"
        env:
          - name: AI_MODEL_BUCKET
            value: "your-ai-model-bucket"
          - name: AI_EXECUTION_MODE
            value: "cloud"  # Set to "edge" for local deployment
        volumeMounts:
          - name: ai-model-volume
            mountPath: /app/model
      volumes:
        - name: ai-model-volume
          persistentVolumeClaim:
            claimName: ai-model-pvc

---
apiVersion: v1
kind: Service
metadata:
  name: unified-model-ai-service
spec:
  selector:
    app: unified-model-ai
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8000
  type: LoadBalancer  # Use NodePort for internal access or ClusterIP for internal-only services

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ai-model-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi  # Adjust based on model size requirements
  storageClassName: managed-premium

---
apiVersion: apps/v1
kind: HorizontalPodAutoscaler
metadata:
  name: unified-model-ai-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: unified-model-ai
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 80

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: unified-model-ai-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: unified-model-ai.yourdomain.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: unified-model-ai-service
            port:
              number: 80

# ------------------------------------------------------------
# ðŸ”¹ **Google Cloud Run for Serverless AI Execution**
# ------------------------------------------------------------
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: unified-model-ai-cloud-run
spec:
  template:
    spec:
      containers:
      - image: gcr.io/YOUR_PROJECT_ID/unified-model-ai:latest
        env:
          - name: AI_MODEL_BUCKET
            value: "your-ai-model-bucket"
        resources:
          limits:
            memory: "2Gi"
            cpu: "1"
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: unified-model-ai-cloud-run-ingress
  annotations:
    networking.k8s.io/rewrite-target: /
spec:
  rules:
  - host: unified-model-ai-cloud.yourdomain.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: unified-model-ai-cloud-run
            port:
              number: 80

# ------------------------------------------------------------
# ðŸ”¹ **Google Compute Engine for On-Demand AI Execution**
# ------------------------------------------------------------
resource "google_compute_instance" "ai_execution_gce" {
  name         = "unified-model-ai-instance"
  machine_type = "n1-standard-8"  # Choose based on AI model requirements
  zone         = "us-central1-a"

  boot_disk {
    initialize_params {
      image = "projects/deeplearning-platform-release/global/images/family/tf-latest-gpu"
    }
  }

  network_interface {
    network = "default"
    access_config {}
  }

  metadata_startup_script = <<-EOF
              #!/bin/bash
              sudo apt update && sudo apt install -y docker docker-compose python3 python3-pip
              sudo systemctl start docker
              sudo systemctl enable docker
              git clone https://github.com/YourOrg/UnifiedModelAI.git /app
              cd /app/deploy
              docker-compose up --build -d
              EOF
}

# ------------------------------------------------------------
# ðŸ”¹ **AI Execution Security Group for GCP Resources**
# ------------------------------------------------------------
resource "google_compute_firewall" "ai_execution_firewall" {
  name    = "ai-execution-firewall"
  network = "default"

  allow {
    protocol = "tcp"
    ports    = ["80", "443", "8000"]
  }

  target_tags = ["ai-execution"]
}

# ------------------------------------------------------------
# ðŸ”¹ **AI Execution Logging and Monitoring**
# ------------------------------------------------------------
resource "google_logging_project_sink" "ai_execution_logs" {
  name        = "ai-execution-logs"
  destination = "storage.googleapis.com/your-log-bucket"
  filter      = "resource.type=\"gce_instance\""

  unique_writer_identity = true
}

# ------------------------------------------------------------
# ðŸ”¹ **AI Execution Failover for Edge AI**
# ------------------------------------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: unified-model-ai-edge-deployment
spec:
  replicas: 1  # Single replica for edge AI execution
  selector:
    matchLabels:
      app: unified-model-ai
  template:
    metadata:
      labels:
        app: unified-model-ai
    spec:
      containers:
      - name: unified-model-ai
        image: gcr.io/YOUR_PROJECT_ID/unified-model-ai:latest  # Replace with your image
        ports:
        - containerPort: 8000
      volumes:
        - name: ai-model-volume
          persistentVolumeClaim:
            claimName: ai-model-pvc
