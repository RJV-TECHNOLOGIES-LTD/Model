version: "3.8"

services:
  ai-inference:
    container_name: ai-inference-service
    build:
      context: ../deployment_automation
      dockerfile: docker_container.dockerfile
    ports:
      - "5000:5000"  # REST API
      - "50051:50051"  # gRPC API
    restart: always
    environment:
      - MODEL_PATH=/app/memory_storage/model_checkpoints.h5
      - CUDA_VISIBLE_DEVICES=0  # Use GPU if available
    volumes:
      - ../memory_storage:/app/memory_storage
    depends_on:
      - database

  database:
    container_name: vector-database
    image: redis:latest  # Replace with `lmdb` or `faiss-server` if needed
    ports:
      - "6379:6379"
    restart: always
    volumes:
      - ../memory_storage:/data
