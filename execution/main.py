import os
import argparse
import logging
import torch
from inference_mechanisms.inference_engine import InferenceEngine
from execution_optimization.scheduler import PhiScheduler
from training_mechanisms.trainer import Trainer
from memory_storage.model_checkpoints import ModelCheckpointManager

# ðŸš€ Logging setup
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# ðŸš€ Auto-detect GPU availability
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
logging.info(f"Using device: {DEVICE}")

class ExecutionEngine:
    def __init__(self, model_path: str, dataset_path: str):
        """Initialize Execution Engine."""
        self.model_path = model_path
        self.dataset_path = dataset_path
        self.scheduler = PhiScheduler()
        self.checkpoint_manager = ModelCheckpointManager()
        self.model = None
        self.trainer = None
        self.inference_engine = None

    def load_model(self):
        """Load AI model and validate integrity."""
        logging.info("Loading model...")
        self.model = self.checkpoint_manager.load_model(self.model_path)
        if self.model is None:
            raise RuntimeError(f"Failed to load model from {self.model_path}")
        logging.info("Model successfully loaded.")

    def train_model(self):
        """Train model with dataset and save checkpoints."""
        if self.model is None:
            raise RuntimeError("Model not loaded. Call load_model() first.")
        logging.info("Initializing training process...")
        self.trainer = Trainer(self.model, self.dataset_path, self.checkpoint_manager)
        self.trainer.train()

    def execute_inference(self, input_data):
        """Perform AI inference."""
        if self.model is None:
            raise RuntimeError("Model not loaded. Call load_model() first.")
        self.inference_engine = InferenceEngine(self.model)
        return self.inference_engine.predict(input_data)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Î¦(a)-Optimized AI Execution Engine")
    parser.add_argument("--model", required=True, help="Path to the AI model file")
    parser.add_argument("--dataset", required=True, help="Path to dataset for training")
    parser.add_argument("--mode", choices=["train", "infer"], required=True, help="Execution mode")

    args = parser.parse_args()

    engine = ExecutionEngine(args.model, args.dataset)
    engine.load_model()

    if args.mode == "train":
        engine.train_model()
    elif args.mode == "infer":
        test_input = {"example": torch.randn(1, 3, 64, 64)}
        result = engine.execute_inference(test_input)
        logging.info(f"Inference Result: {result}")
